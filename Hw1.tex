% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Homework 1},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Homework 1}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{PSTAT Summer 2024}
\author{}
\date{\vspace{-2.5em}Due date: June, 2024}

\begin{document}
\maketitle

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The dataset \emph{trees} contains measurements of Girth (tree
  diameter) in inches, Height in feet, and Volume of timber (in cubic
  feet) of a sample of 31 felled black cherry trees. The following
  commands can be used to read the data into R.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the data set "trees" is contained in the R package "datasets"}
\FunctionTok{require}\NormalTok{(datasets)}
\FunctionTok{head}\NormalTok{(trees)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Girth Height Volume
## 1   8.3     70   10.3
## 2   8.6     65   10.3
## 3   8.8     63   10.2
## 4  10.5     72   16.4
## 5  10.7     81   18.8
## 6  10.8     83   19.7
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  (1pt) Briefly describe the data set trees, i.e., how many observations
  (rows) and how many variables (columns) are there in the data set?
  What are the variable names?

  There are 31 total observations, or rows, and 3 total variables, or
  columns. The variable names are Girth, Height, and Volume.
\item
  (2pts) Use the pairs function to construct a scatter plot matrix of
  the logarithms of Girth, Height and Volume.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(Girth) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(Height) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(Volume), }\AttributeTok{data=}\NormalTok{trees)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Hw1_files/figure-latex/unnamed-chunk-1-1.pdf}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  (2pts) Use the cor function to determine the correlation matrix for
  the three (logged) variables.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(}\FunctionTok{log}\NormalTok{(trees))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            Girth    Height    Volume
## Girth  1.0000000 0.5301949 0.9766649
## Height 0.5301949 1.0000000 0.6486377
## Volume 0.9766649 0.6486377 1.0000000
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\item
  (2pts) Are there missing values?

  No, there are no missing values.
\item
  (2pts) Use the lm function in R to fit the multiple regression model:
  \[log(Volume_i) = \beta_0 + \beta_1 log(Girth_i) + \beta_2 log(Height_i) + \epsilon_i\]
  and print out the summary of the model fit.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(Volume) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(Girth) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(Height), }\AttributeTok{data=}\NormalTok{trees)}
\FunctionTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.168561 -0.048488  0.002431  0.063637  0.129223 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -6.63162    0.79979  -8.292 5.06e-09 ***
## log(Girth)   1.98265    0.07501  26.432  < 2e-16 ***
## log(Height)  1.11712    0.20444   5.464 7.81e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.08139 on 28 degrees of freedom
## Multiple R-squared:  0.9777, Adjusted R-squared:  0.9761 
## F-statistic: 613.2 on 2 and 28 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  (3pts) Create the design matrix (i.e., the matrix of predictor
  variables), X, for the model in (e), and verify that the least squares
  coefficient estimates in the summary output are given by the least
  squares formula: \(\hat\beta = (X^TX)^{-1}X^Ty\).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{times=}\FunctionTok{length}\NormalTok{(trees}\SpecialCharTok{$}\NormalTok{Girth)), }\FunctionTok{log}\NormalTok{(trees}\SpecialCharTok{$}\NormalTok{Girth), }\FunctionTok{log}\NormalTok{(trees}\SpecialCharTok{$}\NormalTok{Height))}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(trees}\SpecialCharTok{$}\NormalTok{Volume))}
\NormalTok{beta.hat }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ x) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ y}
\NormalTok{beta.hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]
## [1,] -6.631617
## [2,]  1.982650
## [3,]  1.117123
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  (3pts) Compute the predicted response values from the fitted
  regression model, the residuals, and an estimate of the error variance
  \(Var(\epsilon) = \sigma^2\).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1        2        3        4        5        6        7        8 
## 2.310270 2.297879 2.308547 2.807900 2.976888 3.022580 2.802931 2.945736 
##        9       10       11       12       13       14       15       16 
## 3.035777 2.981461 3.057130 3.031349 3.031349 2.974906 3.118250 3.246641 
##       17       18       19       20       21       22       23       24 
## 3.401459 3.475068 3.319702 3.218167 3.467691 3.524097 3.478455 3.643019 
##       25       26       27       28       29       30       31 
## 3.754853 3.929478 3.965974 3.983197 3.994242 3.994242 4.355446
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit}\SpecialCharTok{$}\NormalTok{residuals}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            1            2            3            4            5            6 
##  0.021874049  0.034264461  0.013841066 -0.010618992 -0.043031233 -0.041961116 
##            7            8            9           10           11           12 
## -0.055659877 -0.044314840  0.082173329  0.009258910  0.129222704  0.013172999 
##           13           14           15           16           17           18 
##  0.032041483  0.083801431 -0.168561198 -0.146548628  0.119002049 -0.164525292 
##           19           20           21           22           23           24 
## -0.073210648 -0.003299352  0.073268586 -0.067780336  0.113362744  0.002430731 
##           25           26           27           28           29           30 
## -0.002998263  0.085102043  0.054006059  0.082405145 -0.052660573 -0.062416748 
##           31 
## -0.011640695
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigma2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{residuals}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{fit}\SpecialCharTok{$}\NormalTok{df.residual}
\NormalTok{sigma2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.006623692
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Consider the simple linear regression model:
  \[y_i=\beta_0 + \beta_1x_i + \epsilon_i\] \textbf{Part 1:}
  \(\beta_0=0\)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  (3pts) Assume \(\beta_0=0\). What is the interpretation of this
  assumption? What is the implication on the regression line? What does
  the regression line plot look like?

  This means that there is no y-intercept for the regression line, this
  implies that at x=0, y also is 0. This also means that if \(\beta_1\)
  is positive, the line will increase as x increases, and if \(\beta_1\)
  is negative, the line will decrease when x increases
\item
  (4pts) Derive the LS estimate of \(\beta_1\) when \(\beta_0=0\).

  We can estimate each \(\beta\) by minimizing the derivative of SSR
  with respect to each \(\beta\). So if we do this for \(\beta_1\) with
  \(\beta_0=0\):

  \(SSR = \sum_{i=1}^{n}(y_i-\beta_1x_i)^2\)

  \(\frac{\partial}{\partial \beta_1} SSR = \frac{\partial}{\partial \beta_1} \sum_{i=1}^n(y_i-\beta_1x_i)^2\)

  \(\frac{\partial}{\partial \beta_1} SSR = \sum_{i=1}^n(-2y_ix_i+2\beta_1x_i^2)\)

  \(0=\sum_{i=1}^n(-2y_ix_i+2\beta_1x_i^2)\)

  \(\sum_{i=1}^ny_ix_i = \beta_1\sum_{i=1}^nx_i^2\)

  \(\beta_1 = \frac{\sum_{i=1}^ny_ix_i}{\sum_{i=1}^nx_i^2}\)

  \(\hat{\beta_1} = \frac{\sum_{i=1}^ny_ix_i}{\sum_{i=1}^nx_i^2}\)
\item
  (3pts) How can we introduce this assumption within the lm function?\\

  To introduce this assumption to the lm function, we would use
  lm(response \textasciitilde{} 0 + input) so that R the lm function
  knows to not use an intercept.

  \hfill\break
  \textbf{Part 2:} \(\beta_1=0\)\\
\item
  (3pts) For the same model, assume \(\beta_1=0\). What is the
  interpretation of this assumption? What is the implication on the
  regression line? What does the regression line plot look like?

  This means the regression line has a y-intercept, but the line is just
  straight and is not increasing or decreasing. This would imply that
  our prediction for y at every value of x is the same.
\item
  (4pts) Derive the LS estimate of \(\beta_0\) when \(\beta_1=0\).

  We can estimate each \(\beta\) by minimizing the derivative of SSR
  with respect to each \(\beta\). So if we do this for \(\beta_0\) with
  \(\beta_1=0\):

  \(SSR = \sum_{i=1}^{n}(y_i-\beta_0)^2\)

  \(\frac{\partial}{\partial\beta_0} SSR  = \frac{\partial}{\partial\beta_0} \sum_{i=1}^n(yi-\beta_0)^2\)

  \(\frac{\partial}{\partial\beta_0} SSR = \sum_{i=1}^n-2y_i+2\beta_0\)

  \(0 = \sum_{i=1}^n-2y_i+2\beta_0\)

  \(\sum_{i=1}^n y_i = \sum_{i=1}^n \beta_0\)

  \(\beta_0 = \frac{1}{n}\sum_{i=1}^ny_i\)

  \(\hat{\beta_0} = \bar{y}\)
\item
  (3pts) How can we introduce this assumption within the lm function?

  We can introduce this by doing lm(response\textasciitilde1) to
  indicate that \(\beta_1=0\).
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Consider the simple linear regression model:
  \[y_i = \beta_0 + \beta_1x_i + \epsilon_i\]
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  (10pts) Use the LS estimation general result
  \(\hat\beta = (X^TX)^{-1}X^Ty\) to find the explicit estimates for
  \(\beta_0\) and \(\beta_1\).

  For n observations, the design matrix, X, and the response matrix, y
  are:

  \(X = \begin{bmatrix}
  1 & x_1\\
  1 & x_2\\
  \vdots & \vdots\\
  1 & x_n
  \end{bmatrix}\)

  \(y = \begin{bmatrix}
  y_1\\
  y_2\\
  \vdots\\
  y_n
  \end{bmatrix}\)

  First we will calculate \(X^TX\):

  \(X^TX = \begin{bmatrix}
  1 & 1 & \dots & 1\\
  x_1 & x_2 & \dots & x_n
  \end{bmatrix}\begin{bmatrix}
  1 & x_1\\
  1 & x_2\\
  \vdots & \vdots\\
  1 & x_n
  \end{bmatrix}=
  \begin{bmatrix}
  n & \sum_{i=1}^nx_i\\
  \sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2
  \end{bmatrix}\)

  Now I can compute \(X^Ty\):

  \(X^Ty = \begin{bmatrix}
  1 & 1 & \dots & 1\\
  x_1 & x_2 & \dots & x_n
  \end{bmatrix}\begin{bmatrix}
  y_1\\
  y_2\\
  \vdots\\
  y_n
  \end{bmatrix} = \begin{bmatrix}
  \sum_{i=1}^ny_i\\
  \sum_{i=1}^nx_iy_i
  \end{bmatrix}\)

  The inverse of \(X^TX\) is:

  \((X^TX)^{-1} =      \frac{1}{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\begin{bmatrix}
  \sum_{i=1}^nx_i^2 & -\sum_{i=1}^nx_i\\
  -\sum_{i=1}^nx_i & n
  \end{bmatrix}\)

  Finally we can compute \(\hat{\beta}\):

  \(\hat{\beta} = (X^TX)^{-1}X^Ty = \frac{1}{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\begin{bmatrix}
  \sum_{i=1}^nx_i^2 & -\sum_{i=1}^nx_i\\
  -\sum_{i=1}^nx_i & n
  \end{bmatrix}\begin{bmatrix}
  \sum_{i=1}^ny_i\\
  \sum_{i=1}^nx_iy_i
  \end{bmatrix}\)

  \(\hat{\beta} = \frac{1}{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\begin{bmatrix}
  \sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_iy_i\\
  =\sum_{i=1}^nx_i\sum_{i=1}^ny_i+n\sum_{i=1}^nx_iy_i
  \end{bmatrix}\)

  This gives us:

  \(\hat{\beta_0}=\frac{\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_iy_i}{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\)

  \(\hat{\beta_1}=\frac{n\sum_{i=1}^mx_iy_i-\sum_{i=1}^nx_i\sum_{i=1}^ny_i}{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\)

  These can finally be simplified to be:

  \(\hat{\beta_1} = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\)

  \(\hat{\beta_0} = \bar{y}-\hat{\beta_1}\bar{x}\)
\item
  (5pts) Show that the LS estimates \(\hat\beta_0\) and \(\hat\beta_1\)
  are unbiased estimates for \(\beta_0\) and \(\beta_1\) respectively.

  I will first show that \(\hat{\beta_1}\) is unbiased:

  \(y_i=\beta_0+\beta_1x_i+\epsilon_i\)

  \(\hat{\beta_1} = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{\sum_{i=1}^n(x_i-\bar{x})(\beta_0+\beta_1x_i+\epsilon_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\)

  \(\hat{\beta_1} = \frac{\sum_{i=1}^n(x_i-\bar{x})(\beta_0+\beta_1x_i+\epsilon_i-\beta_0-\beta_1\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}\)

  \(=\frac{\sum_{i=1}^n(x_i-\bar{x})(\beta_1(x_i-\bar{x})+\epsilon_i)}{\sum_{i=1}^n(x_i-\bar{x})^2}\)

  \(=\beta_1\frac{\sum_{i=1}^n(x_i-\bar{x})^2}{\sum_{i=1}^n(x_i-\bar{x})^2}+\frac{\sum_{i=1}^n(x_i-\bar{x})\epsilon_i}{\sum_{i=1}^n(x_i-\bar{x})^2}\)

  \(=\beta_1+\frac{\sum_{i=1}^n(x_i-\bar{x})\epsilon_i}{\sum_{i=1}^n(x_i-\bar{x})^2}\)

  Because \(E[\epsilon_i]=0\), it is east to see that
  \(E[\hat{\beta_1}]=\beta_1\) and thus it is unbiased.

  Now for \(\hat{\beta_0}\):

  \(\hat{\beta_0}=\bar{y}-\hat{\beta_1}-\bar{x}\)

  \(E[\hat{\beta_0}]=E[\bar{y}-\hat{\beta_1}\bar{x}]\)

  \(=E[\bar{y}]-E[\hat{\beta_1}\bar{x}]\)

  \(=E[\bar{y}-\bar{x}E[\hat{\beta_1}]\)

  \(=E[\bar{y}]-\bar{x}\beta_1\)

  \(=E[\frac{1}{n}\sum_{i=1}^ny_i]-\bar{x}\beta_1\)

  \(=E[\frac{1}{n}\sum_{i=1}^n(\beta_0+\beta_1x_i+\epsilon_i)]-\bar{x}\beta_1\)

  \(=E[\beta_0+\beta_1\bar{x}+\frac{1}{n}\sum_{i=1}^n(\epsilon_i)]-\bar{x}\beta_1\)

  \(=\beta_0+\beta_1\bar{x}-\bar{x}\beta_1\)

  \(=\beta_0\)

  Thus, \(\hat{\beta_0\) is also unbiased.
\end{enumerate}

\end{document}
